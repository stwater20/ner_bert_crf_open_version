{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnRTOt6m79xi",
        "outputId": "e0a7df88-2d5b-4898-ed87-6bf8411f991b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ner_bert_crf_open_version\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ner_bert_crf_open_version/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.1\n",
        "!pip install transformers==4.24.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH0gc38X8B9n",
        "outputId": "65288ecb-bcd3-4e2b-d382-8f587bc9b405"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.9/dist-packages (1.12.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.12.1) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.24.0 in /usr/local/lib/python3.9/dist-packages (4.24.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.24.0) (0.13.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.24.0) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel,BertPreTrainedModel,AutoTokenizer\n",
        "import os\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hhXsxShx8EiW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "參數設定"
      ],
      "metadata": {
        "id": "RzXtPUki-tGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 256\n",
        "batch_size = 16\n",
        "gradient_accumulation_steps = 1\n",
        "total_train_epochs = 50\n",
        "\n",
        "output_dir = r'./outputs//'\n",
        "def get_data_dir(local_path=r\".//\", server_path=\"ner_bert_crf_open_version\"):\n",
        "    return r'.//'\n"
      ],
      "metadata": {
        "id": "2Ro2stNM-OgA"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "處理資料集的地方"
      ],
      "metadata": {
        "id": "xX6V3OWk-l69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for NER.\"\"\"\n",
        "    def __init__(self, guid, words, labels):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "          guid: Unique id for the example(a sentence or a pair of sentences).\n",
        "          words: list of words of sentence\n",
        "          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        # list of words of the sentence,example: [EU, rejects, German, call, to, boycott, British, lamb .]\n",
        "        self.words = words\n",
        "        # list of label sequence of the sentence,like: [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]\n",
        "        self.labels = labels\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\n",
        "    result of convert_examples_to_features(InputExample)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.predict_mask = predict_mask\n",
        "        self.label_ids = label_ids\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_data(cls, input_file):\n",
        "        \"\"\"\n",
        "        Reads a BIO data.\n",
        "        \"\"\"\n",
        "        with open(input_file) as f:\n",
        "            # out_lines = []\n",
        "            out_lists = []\n",
        "            entries = f.read().strip().split(\"\\n\\n\")\n",
        "            for entry in entries:\n",
        "                words = []\n",
        "                ner_labels = []\n",
        "                pos_tags = []\n",
        "                bio_pos_tags = []\n",
        "                for line in entry.splitlines():\n",
        "                    pieces = line.strip().split()\n",
        "                    if len(pieces) < 1:\n",
        "                        continue\n",
        "                    word = pieces[0]\n",
        "                    # if word == \"-DOCSTART-\" or word == '':\n",
        "                    #     continue\n",
        "                    words.append(word)\n",
        "                    #pos_tags.append(pieces[1])\n",
        "                    #bio_pos_tags.append(pieces[2])\n",
        "                    ner_labels.append(pieces[-1])\n",
        "                # sentence = ' '.join(words)\n",
        "                # ner_seq = ' '.join(ner_labels)\n",
        "                # pos_tag_seq = ' '.join(pos_tags)\n",
        "                # bio_pos_tag_seq = ' '.join(bio_pos_tags)\n",
        "                # out_lines.append([sentence, pos_tag_seq, bio_pos_tag_seq, ner_seq])\n",
        "                # out_lines.append([sentence, ner_seq])\n",
        "                out_lists.append([words,pos_tags,bio_pos_tags,ner_labels])\n",
        "        return out_lists\n",
        "    \n",
        "    \n",
        "class DNRTI_DataProcessor(DataProcessor):\n",
        "    '''\n",
        "    DNRTI_-2003\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self._label_types =  [ 'X', '[CLS]', '[SEP]', 'O', 'B-Area', 'B-Exp', 'B-Features', 'B-HackOrg', 'B-Idus', 'B-OffAct','B-Org', 'B-Purp', 'B-SamFile','B-SecTeam','B-Time','B-Tool','B-Way','I-Area','I-Exp','I-Features','I-HackOrg','I-Idus','I-OffAct','I-Org','I-Purp','I-SamFile','I-SecTeam','I-Time','I-Tool','I-Way']\n",
        "        self._num_labels = len(self._label_types)\n",
        "        self._label_map = {label: i for i,\n",
        "                           label in enumerate(self._label_types)}\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_data(os.path.join(data_dir, \"train.txt\")))\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_data(os.path.join(data_dir, \"valid.txt\")))\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_data(os.path.join(data_dir, \"test.txt\")))\n",
        "    def get_predict_examples(self, data_dir,predict_string):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "    def get_labels(self):\n",
        "        return self._label_types\n",
        "\n",
        "    def get_num_labels(self):\n",
        "        return self.get_num_labels\n",
        "\n",
        "    def get_label_map(self):\n",
        "        return self._label_map\n",
        "\n",
        "    def get_start_label_id(self):\n",
        "        return self._label_map['[CLS]']\n",
        "\n",
        "    def get_stop_label_id(self):\n",
        "        return self._label_map['[SEP]']\n",
        "\n",
        "    def _create_examples(self, all_lists):\n",
        "        examples = []\n",
        "        for (i, one_lists) in enumerate(all_lists):\n",
        "            guid = i\n",
        "            words = one_lists[0]\n",
        "            labels = one_lists[-1]\n",
        "            examples.append(InputExample(\n",
        "                guid=guid, words=words, labels=labels))\n",
        "        return examples\n",
        "\n",
        "    def _create_examples2(self, lines):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = i\n",
        "            text = line[0]\n",
        "            ner_label = line[-1]\n",
        "            examples.append(InputExample(\n",
        "                guid=guid, text_a=text, labels_a=ner_label))\n",
        "        return examples\n",
        "    \n",
        "def example2feature(example, tokenizer, label_map, max_seq_length):\n",
        "\n",
        "    add_label = 'X'\n",
        "    # tokenize_count = []\n",
        "    tokens = ['[CLS]']\n",
        "    predict_mask = [0]\n",
        "    label_ids = [label_map['[CLS]']]\n",
        "    for i, w in enumerate(example.words):\n",
        "        # use bertTokenizer to split words\n",
        "        # 1996-08-22 => 1996 - 08 - 22\n",
        "        # sheepmeat => sheep ##me ##at\n",
        "        sub_words = tokenizer.tokenize(w)\n",
        "        if not sub_words:\n",
        "            sub_words = ['[UNK]']\n",
        "        # tokenize_count.append(len(sub_words))\n",
        "        tokens.extend(sub_words)\n",
        "        for j in range(len(sub_words)):\n",
        "            if j == 0:\n",
        "                predict_mask.append(1)\n",
        "                label_ids.append(label_map[example.labels[i]])\n",
        "            else:\n",
        "                # '##xxx' -> 'X' (see bert paper)\n",
        "                predict_mask.append(0)\n",
        "                label_ids.append(label_map[add_label])\n",
        "\n",
        "    # truncate\n",
        "    if len(tokens) > max_seq_length - 1:\n",
        "        print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n",
        "        tokens = tokens[0:(max_seq_length - 1)]\n",
        "        predict_mask = predict_mask[0:(max_seq_length - 1)]\n",
        "        label_ids = label_ids[0:(max_seq_length - 1)]\n",
        "    tokens.append('[SEP]')\n",
        "    predict_mask.append(0)\n",
        "    label_ids.append(label_map['[SEP]'])\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    segment_ids = [0] * len(input_ids)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    feat=InputFeatures(\n",
        "                # guid=example.guid,\n",
        "                # tokens=tokens,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                segment_ids=segment_ids,\n",
        "                predict_mask=predict_mask,\n",
        "                label_ids=label_ids)\n",
        "\n",
        "    return feat\n",
        "\n",
        "class NerDataset(data.Dataset):\n",
        "    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n",
        "        self.examples=examples\n",
        "        self.tokenizer=tokenizer\n",
        "        self.label_map=label_map\n",
        "        self.max_seq_length=max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feat=example2feature(self.examples[idx], self.tokenizer, self.label_map, max_seq_length)\n",
        "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n",
        "\n",
        "    @classmethod\n",
        "    def pad(cls, batch):\n",
        "\n",
        "        seqlen_list = [len(sample[0]) for sample in batch]\n",
        "        maxlen = np.array(seqlen_list).max()\n",
        "\n",
        "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
        "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
        "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
        "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
        "        predict_mask_list = torch.ByteTensor(f(3, maxlen))\n",
        "        label_ids_list = torch.LongTensor(f(4, maxlen))\n",
        "\n",
        "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list\n"
      ],
      "metadata": {
        "id": "8MPOpQid-Y2C"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 讀 DNRTI 資料\n",
        "\n",
        "data_dir = os.path.join(get_data_dir(), 'datasets/DNRTI/')\n",
        "DNRTI_Processor = DNRTI_DataProcessor()\n",
        "label_list = DNRTI_Processor.get_labels()\n",
        "label_map = DNRTI_Processor.get_label_map()\n",
        "train_examples = DNRTI_Processor.get_train_examples(data_dir)\n",
        "dev_examples = DNRTI_Processor.get_dev_examples(data_dir)\n",
        "test_examples = DNRTI_Processor.get_test_examples(data_dir)\n",
        "total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
        "\n",
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\"% len(train_examples))\n",
        "print(\"  Batch size = %d\"% batch_size)\n",
        "print(\"  Num steps = %d\"% total_train_steps)\n",
        "\n",
        "\n",
        "bert_model_scale = 'bert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_scale, do_lower_case=True)\n",
        "\n",
        "\n",
        "train_dataset = NerDataset(train_examples,tokenizer,label_map,max_seq_length)\n",
        "dev_dataset = NerDataset(dev_examples,tokenizer,label_map,max_seq_length)\n",
        "test_dataset = NerDataset(test_examples,tokenizer,label_map,max_seq_length)\n",
        "\n",
        "train_dataloader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=4,\n",
        "                                collate_fn=NerDataset.pad)\n",
        "\n",
        "dev_dataloader = data.DataLoader(dataset=dev_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=4,\n",
        "                                collate_fn=NerDataset.pad)\n",
        "\n",
        "test_dataloader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=4,\n",
        "                                collate_fn=NerDataset.pad)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUrvYXiR-vzz",
        "outputId": "6cabe698-7ce0-4f91-ef3c-71009171ea1a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 5251\n",
            "  Batch size = 16\n",
            "  Num steps = 16409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "2CleaZ1s-5cK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    '''\n",
        "    0,1,2,3 are [CLS],[SEP],[X],O\n",
        "    '''\n",
        "    ignore_id=3\n",
        "\n",
        "    num_proposed = len(y_pred[y_pred>ignore_id])\n",
        "    num_correct = (np.logical_and(y_true==y_pred, y_true>ignore_id)).sum()\n",
        "    num_gold = len(y_true[y_true>ignore_id])\n",
        "\n",
        "    try:\n",
        "        precision = num_correct / num_proposed\n",
        "    except ZeroDivisionError:\n",
        "        precision = 1.0\n",
        "\n",
        "    try:\n",
        "        recall = num_correct / num_gold\n",
        "    except ZeroDivisionError:\n",
        "        recall = 1.0\n",
        "\n",
        "    try:\n",
        "        f1 = 2*precision*recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        if precision*recall==0:\n",
        "            f1=1.0\n",
        "        else:\n",
        "            f1=0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "def log_sum_exp_1vec(vec):  # shape(1,m)\n",
        "    max_score = vec[0, np.argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n",
        "    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n",
        "\n",
        "def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n",
        "    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))"
      ],
      "metadata": {
        "id": "2XWZYe9d_grY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT-CRF Model (CRF 寫在 train 上)"
      ],
      "metadata": {
        "id": "a3eSEUMG_izT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build BERT_CRF_NER model\n",
        "\n",
        "class BERT_CRF_NER(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n",
        "        super(BERT_CRF_NER, self).__init__()\n",
        "        self.hidden_size = 768\n",
        "        self.start_label_id = start_label_id\n",
        "        self.stop_label_id = stop_label_id\n",
        "        self.num_labels = num_labels\n",
        "        # self.max_seq_length = max_seq_length\n",
        "        self.batch_size = batch_size\n",
        "        self.device=device\n",
        "\n",
        "        # use pretrainded BertModel\n",
        "        self.bert = bert_model\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "        # Maps the output of the bert into label space.\n",
        "        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.num_labels, self.num_labels))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n",
        "        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n",
        "        # so this enforcement is likely unimportant)\n",
        "        self.transitions.data[start_label_id, :] = -10000\n",
        "        self.transitions.data[:, stop_label_id] = -10000\n",
        "\n",
        "        nn.init.xavier_uniform_(self.hidden2label.weight)\n",
        "        nn.init.constant_(self.hidden2label.bias, 0.0)\n",
        "        # self.apply(self.init_bert_weights)\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        '''\n",
        "        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX\n",
        "        '''\n",
        "\n",
        "        # T = self.max_seq_length\n",
        "        T = feats.shape[1]\n",
        "        batch_size = feats.shape[0]\n",
        "\n",
        "        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n",
        "        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
        "        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n",
        "        # self.start_label has all of the score. it is log,0 is p=1\n",
        "        log_alpha[:, 0, self.start_label_id] = 0\n",
        "\n",
        "        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n",
        "        # feats is the probability of emission, feat.shape=(1,tag_size)\n",
        "        for t in range(1, T):\n",
        "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
        "\n",
        "        # log_prob of all barX\n",
        "        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n",
        "        return log_prob_all_barX\n",
        "\n",
        "    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n",
        "        '''\n",
        "        sentances -> word embedding -> lstm -> MLP -> feats\n",
        "        '''\n",
        "        bert_seq_out, _ = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask,return_dict=False)\n",
        "        bert_seq_out = self.dropout(bert_seq_out)\n",
        "        bert_feats = self.hidden2label(bert_seq_out)\n",
        "        return bert_feats\n",
        "\n",
        "    def _score_sentence(self, feats, label_ids):\n",
        "        '''\n",
        "        Gives the score of a provided label sequence\n",
        "        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
        "        '''\n",
        "\n",
        "        # T = self.max_seq_length\n",
        "        T = feats.shape[1]\n",
        "        batch_size = feats.shape[0]\n",
        "\n",
        "        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
        "        batch_transitions = batch_transitions.flatten(1)\n",
        "\n",
        "        score = torch.zeros((feats.shape[0],1)).to(device)\n",
        "        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n",
        "        for t in range(1, T):\n",
        "            score = score + \\\n",
        "                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n",
        "                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        '''\n",
        "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
        "        '''\n",
        "\n",
        "        # T = self.max_seq_length\n",
        "        T = feats.shape[1]\n",
        "        batch_size = feats.shape[0]\n",
        "\n",
        "        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
        "\n",
        "        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
        "        log_delta[:, 0, self.start_label_id] = 0\n",
        "\n",
        "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
        "        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n",
        "        for t in range(1, T):\n",
        "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
        "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
        "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
        "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
        "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
        "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
        "\n",
        "        # trace back\n",
        "        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n",
        "\n",
        "        # max p(z1:t,all_x|theta)\n",
        "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
        "\n",
        "        for t in range(T-2, -1, -1):\n",
        "            # choose the state of z_t according the state choosed of z_t+1.\n",
        "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
        "\n",
        "        return max_logLL_allz_allx, path\n",
        "\n",
        "    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n",
        "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
        "        forward_score = self._forward_alg(bert_feats)\n",
        "        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
        "        gold_score = self._score_sentence(bert_feats, label_ids)\n",
        "        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n",
        "        return torch.mean(forward_score - gold_score)\n",
        "\n",
        "    # this forward is just for predict, not for train\n",
        "    # dont confuse this with _forward_alg above.\n",
        "    def forward(self, input_ids, segment_ids, input_mask):\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, label_seq_ids = self._viterbi_decode(bert_feats)\n",
        "        return score, label_seq_ids\n",
        "\n",
        "\n",
        "start_label_id = DNRTI_Processor.get_start_label_id()\n",
        "stop_label_id = DNRTI_Processor.get_stop_label_id()\n",
        "bert_model = BertModel.from_pretrained('bert-base-cased')\n",
        "model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(label_list), max_seq_length, batch_size, device)\n",
        "start_epoch = 0\n",
        "valid_acc_prev = 0\n",
        "valid_f1_prev = 0\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzaOQgKu_GXE",
        "outputId": "f20e7ee7-9b71-4014-ec1d-580b09c2eec6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_CRF_NER(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (hidden2label): Linear(in_features=768, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 超參數\n",
        "\n",
        "\n",
        "lr0_crf_fc = 8e-5\n",
        "learning_rate0 = 5e-5\n",
        "weight_decay_crf_fc = 5e-6 #0.005\n",
        "weight_decay_finetune = 1e-5 #0.01\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "new_param = ['transitions', 'hidden2label.weight', 'hidden2label.bias']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) \\\n",
        "        and not any(nd in n for nd in new_param)], 'weight_decay': weight_decay_finetune},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) \\\n",
        "        and not any(nd in n for nd in new_param)], 'weight_decay': 0.0},\n",
        "    {'params': [p for n, p in param_optimizer if n in ('transitions','hidden2label.weight')] \\\n",
        "        , 'lr':lr0_crf_fc, 'weight_decay': weight_decay_crf_fc},\n",
        "    {'params': [p for n, p in param_optimizer if n == 'hidden2label.bias'] \\\n",
        "        , 'lr':lr0_crf_fc, 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "#optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate0)\n"
      ],
      "metadata": {
        "id": "39GMEKcP_sZv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import time\n",
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x\n",
        "def evaluate(model, predict_dataloader, batch_size, epoch_th, dataset_name):\n",
        "    # print(\"***** Running prediction *****\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total=0\n",
        "    correct=0\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch in predict_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
        "            _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
        "            # _, predicted = torch.max(out_scores, -1)\n",
        "            valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
        "            valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
        "            all_preds.extend(valid_predicted.tolist())\n",
        "            all_labels.extend(valid_label_ids.tolist())\n",
        "            # print(len(valid_label_ids),len(valid_predicted),len(valid_label_ids)==len(valid_predicted))\n",
        "            total += len(valid_label_ids)\n",
        "            correct += valid_predicted.eq(valid_label_ids).sum().item()\n",
        "\n",
        "    test_acc = correct/total\n",
        "    precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n",
        "    end = time.time()\n",
        "    print('Epoch:%d, Acc:%.2f, Precision: %.2f, Recall: %.2f, F1: %.2f on %s, Spend:%.3f minutes for evaluation' % (epoch_th, 100.*test_acc, 100.*precision, 100.*recall, 100.*f1, dataset_name,(end-start)/60.0))\n",
        "    print('--------------------------------------------------------------')\n",
        "    return test_acc, f1\n",
        "global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n",
        "\n",
        "warmup_proportion = 0.1"
      ],
      "metadata": {
        "id": "VdLGNEW5_22_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練跟評估\n",
        "forward 與 backward 上使用到 CRF 算法"
      ],
      "metadata": {
        "id": "megZSyWF_9YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training Model and evaluation\n",
        "\n",
        "for epoch in range(start_epoch, total_train_epochs):\n",
        "    tr_loss = 0\n",
        "    train_start = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
        "\n",
        "        neg_log_likelihood = model.neg_log_likelihood(input_ids, segment_ids, input_mask, label_ids)\n",
        "\n",
        "        if gradient_accumulation_steps > 1:\n",
        "            neg_log_likelihood = neg_log_likelihood / gradient_accumulation_steps\n",
        "\n",
        "        neg_log_likelihood.backward()\n",
        "\n",
        "        tr_loss += neg_log_likelihood.item()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            # modify learning rate with special warm up BERT uses\n",
        "            lr_this_step = learning_rate0 * warmup_linear(global_step_th/total_train_steps, warmup_proportion)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_this_step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step_th += 1\n",
        "\n",
        "        # print(\"Epoch:{}-{}/{}, Negative loglikelihood: {} \".format(epoch, step, len(train_dataloader), neg_log_likelihood.item()))\n",
        "\n",
        "    print('--------------------------------------------------------------')\n",
        "    print(\"Epoch:{} completed, Total training's Loss: {}, Spend: {}m\".format(epoch, tr_loss, (time.time() - train_start)/60.0))\n",
        "    valid_acc, valid_f1 = evaluate(model, dev_dataloader, batch_size, epoch, 'Valid_set')\n",
        "\n",
        "    # Save a checkpoint\n",
        "    if valid_f1 > valid_f1_prev:\n",
        "        # model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_acc': valid_acc,\n",
        "            'valid_f1': valid_f1, 'max_seq_length': max_seq_length, 'lower_case': False},\n",
        "                    os.path.join(output_dir, 'ner_bert_crf_checkpoint.pt'))\n",
        "        valid_f1_prev = valid_f1\n",
        "\n",
        "evaluate(model, test_dataloader, batch_size, total_train_epochs-1, 'Test_set')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR1G6D10_47T",
        "outputId": "edae8474-95fb-4476-f4df-773321090150"
      },
      "execution_count": 49,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------\n",
            "Epoch:0 completed, Total training's Loss: 3102469.2255859375, Spend: 1.4820603410402933m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a15976b9ee89>:24: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n",
            "  valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
            "<ipython-input-48-a15976b9ee89>:25: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n",
            "  valid_label_ids = torch.masked_select(label_ids, predict_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0, Acc:82.30, Precision: 39.15, Recall: 17.86, F1: 24.53 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:1 completed, Total training's Loss: 3070454.4965820312, Spend: 1.3785933136940003m\n",
            "Epoch:1, Acc:86.53, Precision: 56.30, Recall: 57.24, F1: 56.77 on Valid_set, Spend:0.065 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:2 completed, Total training's Loss: 3064123.87109375, Spend: 1.3691362659136455m\n",
            "Epoch:2, Acc:89.47, Precision: 70.49, Recall: 60.58, F1: 65.16 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:3 completed, Total training's Loss: 3062122.7626953125, Spend: 1.3744303107261657m\n",
            "Epoch:3, Acc:89.98, Precision: 66.49, Recall: 76.28, F1: 71.05 on Valid_set, Spend:0.064 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:4 completed, Total training's Loss: 3061919.0737304688, Spend: 1.367233896255493m\n",
            "Epoch:4, Acc:92.21, Precision: 74.21, Recall: 78.38, F1: 76.24 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:5 completed, Total training's Loss: 3055998.910888672, Spend: 1.3682778358459473m\n",
            "Epoch:5, Acc:93.23, Precision: 77.28, Recall: 83.27, F1: 80.16 on Valid_set, Spend:0.054 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:6 completed, Total training's Loss: 3050445.7431640625, Spend: 1.3869648019472758m\n",
            "Epoch:6, Acc:93.32, Precision: 76.82, Recall: 85.42, F1: 80.89 on Valid_set, Spend:0.064 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:7 completed, Total training's Loss: 3042626.4521484375, Spend: 1.4012268861134847m\n",
            "Epoch:7, Acc:93.99, Precision: 79.67, Recall: 85.21, F1: 82.35 on Valid_set, Spend:0.056 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:8 completed, Total training's Loss: 3041976.1342773438, Spend: 1.396640141805013m\n",
            "Epoch:8, Acc:94.46, Precision: 80.46, Recall: 86.30, F1: 83.28 on Valid_set, Spend:0.060 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:9 completed, Total training's Loss: 3039129.3623046875, Spend: 1.3880638639132181m\n",
            "Epoch:9, Acc:94.72, Precision: 83.52, Recall: 84.36, F1: 83.94 on Valid_set, Spend:0.057 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:10 completed, Total training's Loss: 3027169.0400390625, Spend: 1.380708885192871m\n",
            "Epoch:10, Acc:94.47, Precision: 81.07, Recall: 85.97, F1: 83.45 on Valid_set, Spend:0.054 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:11 completed, Total training's Loss: 3022955.6411132812, Spend: 1.353533927599589m\n",
            "Epoch:11, Acc:93.85, Precision: 79.06, Recall: 86.09, F1: 82.42 on Valid_set, Spend:0.061 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:12 completed, Total training's Loss: 3017013.7265625, Spend: 1.3622528870900472m\n",
            "Epoch:12, Acc:94.98, Precision: 85.55, Recall: 84.48, F1: 85.01 on Valid_set, Spend:0.062 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:13 completed, Total training's Loss: 3009377.115234375, Spend: 1.376029666264852m\n",
            "Epoch:13, Acc:94.61, Precision: 82.15, Recall: 86.37, F1: 84.20 on Valid_set, Spend:0.054 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:14 completed, Total training's Loss: 2997648.6831054688, Spend: 1.3584047913551331m\n",
            "Epoch:14, Acc:94.37, Precision: 80.66, Recall: 86.37, F1: 83.41 on Valid_set, Spend:0.058 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:15 completed, Total training's Loss: 2997384.4169921875, Spend: 1.3653531193733215m\n",
            "Epoch:15, Acc:94.71, Precision: 81.38, Recall: 87.49, F1: 84.33 on Valid_set, Spend:0.063 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:16 completed, Total training's Loss: 2990813.8344726562, Spend: 1.3541959881782533m\n",
            "Epoch:16, Acc:94.78, Precision: 82.06, Recall: 86.40, F1: 84.17 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:17 completed, Total training's Loss: 2984773.7504882812, Spend: 1.360651425520579m\n",
            "Epoch:17, Acc:94.65, Precision: 80.97, Recall: 87.06, F1: 83.90 on Valid_set, Spend:0.054 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:18 completed, Total training's Loss: 2976144.5517578125, Spend: 1.354291852315267m\n",
            "Epoch:18, Acc:94.63, Precision: 81.06, Recall: 87.61, F1: 84.21 on Valid_set, Spend:0.065 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:19 completed, Total training's Loss: 2968161.7880859375, Spend: 1.3571130752563476m\n",
            "Epoch:19, Acc:94.84, Precision: 82.98, Recall: 86.18, F1: 84.55 on Valid_set, Spend:0.058 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:20 completed, Total training's Loss: 2958882.544921875, Spend: 1.3559154391288757m\n",
            "Epoch:20, Acc:94.77, Precision: 81.76, Recall: 87.37, F1: 84.47 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:21 completed, Total training's Loss: 2952131.1450195312, Spend: 1.344585394859314m\n",
            "Epoch:21, Acc:94.91, Precision: 83.99, Recall: 85.85, F1: 84.91 on Valid_set, Spend:0.064 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:22 completed, Total training's Loss: 2947812.3779296875, Spend: 1.35852423508962m\n",
            "Epoch:22, Acc:94.69, Precision: 81.53, Recall: 87.25, F1: 84.29 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:23 completed, Total training's Loss: 2940002.4233398438, Spend: 1.3415416439374288m\n",
            "Epoch:23, Acc:94.65, Precision: 81.95, Recall: 86.97, F1: 84.38 on Valid_set, Spend:0.063 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:24 completed, Total training's Loss: 2930566.1284179688, Spend: 1.3545758485794068m\n",
            "Epoch:24, Acc:94.53, Precision: 80.58, Recall: 87.31, F1: 83.81 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:25 completed, Total training's Loss: 2921254.5541992188, Spend: 1.341170601050059m\n",
            "Epoch:25, Acc:94.71, Precision: 81.81, Recall: 87.25, F1: 84.44 on Valid_set, Spend:0.064 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:26 completed, Total training's Loss: 2916847.2084960938, Spend: 1.351937758922577m\n",
            "Epoch:26, Acc:94.63, Precision: 81.50, Recall: 86.58, F1: 83.96 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:27 completed, Total training's Loss: 2907294.7827148438, Spend: 1.343782941500346m\n",
            "Epoch:27, Acc:94.47, Precision: 80.63, Recall: 86.70, F1: 83.55 on Valid_set, Spend:0.062 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:28 completed, Total training's Loss: 2900574.16015625, Spend: 1.3478086829185485m\n",
            "Epoch:28, Acc:94.79, Precision: 82.01, Recall: 87.37, F1: 84.61 on Valid_set, Spend:0.054 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:29 completed, Total training's Loss: 2897015.44921875, Spend: 1.3344805240631104m\n",
            "Epoch:29, Acc:94.49, Precision: 81.23, Recall: 86.33, F1: 83.70 on Valid_set, Spend:0.061 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:30 completed, Total training's Loss: 2892746.837890625, Spend: 1.3566972335179648m\n",
            "Epoch:30, Acc:94.91, Precision: 83.92, Recall: 85.91, F1: 84.90 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:31 completed, Total training's Loss: 2885017.811767578, Spend: 1.3416699012120565m\n",
            "Epoch:31, Acc:94.58, Precision: 81.61, Recall: 86.40, F1: 83.94 on Valid_set, Spend:0.058 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:32 completed, Total training's Loss: 2878519.2666015625, Spend: 1.3560626943906149m\n",
            "Epoch:32, Acc:94.80, Precision: 83.17, Recall: 86.46, F1: 84.78 on Valid_set, Spend:0.056 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:33 completed, Total training's Loss: 2877306.34375, Spend: 1.3475196957588196m\n",
            "Epoch:33, Acc:94.67, Precision: 81.67, Recall: 87.25, F1: 84.36 on Valid_set, Spend:0.057 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:34 completed, Total training's Loss: 2869477.5029296875, Spend: 1.362235947450002m\n",
            "Epoch:34, Acc:94.66, Precision: 82.49, Recall: 86.12, F1: 84.27 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:35 completed, Total training's Loss: 2866996.1069335938, Spend: 1.350282613436381m\n",
            "Epoch:35, Acc:94.80, Precision: 82.37, Recall: 86.82, F1: 84.54 on Valid_set, Spend:0.058 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:36 completed, Total training's Loss: 2864318.267578125, Spend: 1.368574114640554m\n",
            "Epoch:36, Acc:94.66, Precision: 82.63, Recall: 86.40, F1: 84.47 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:37 completed, Total training's Loss: 2856786.4736328125, Spend: 1.3522003690401714m\n",
            "Epoch:37, Acc:94.84, Precision: 82.62, Recall: 86.91, F1: 84.71 on Valid_set, Spend:0.060 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:38 completed, Total training's Loss: 2853564.6103515625, Spend: 1.3670868555704752m\n",
            "Epoch:38, Acc:94.65, Precision: 81.80, Recall: 86.79, F1: 84.22 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:39 completed, Total training's Loss: 2849875.2397460938, Spend: 1.3530680894851685m\n",
            "Epoch:39, Acc:94.56, Precision: 81.32, Recall: 86.85, F1: 83.99 on Valid_set, Spend:0.061 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:40 completed, Total training's Loss: 2844900.8984375, Spend: 1.357045849164327m\n",
            "Epoch:40, Acc:94.77, Precision: 82.90, Recall: 85.97, F1: 84.41 on Valid_set, Spend:0.054 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:41 completed, Total training's Loss: 2842638.8627929688, Spend: 1.3563422401746115m\n",
            "Epoch:41, Acc:94.70, Precision: 82.34, Recall: 86.52, F1: 84.38 on Valid_set, Spend:0.059 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:42 completed, Total training's Loss: 2836518.4921875, Spend: 1.361812674999237m\n",
            "Epoch:42, Acc:94.79, Precision: 83.08, Recall: 86.06, F1: 84.55 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:43 completed, Total training's Loss: 2840681.37890625, Spend: 1.346283233165741m\n",
            "Epoch:43, Acc:94.50, Precision: 80.75, Recall: 87.37, F1: 83.93 on Valid_set, Spend:0.058 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:44 completed, Total training's Loss: 2836322.1748046875, Spend: 1.3586241642634074m\n",
            "Epoch:44, Acc:94.65, Precision: 81.71, Recall: 86.70, F1: 84.13 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:45 completed, Total training's Loss: 2839333.533203125, Spend: 1.3380398631095887m\n",
            "Epoch:45, Acc:94.88, Precision: 83.51, Recall: 86.09, F1: 84.78 on Valid_set, Spend:0.057 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:46 completed, Total training's Loss: 2836282.4697265625, Spend: 1.352865493297577m\n",
            "Epoch:46, Acc:94.64, Precision: 81.84, Recall: 86.61, F1: 84.15 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:47 completed, Total training's Loss: 2836530.9404296875, Spend: 1.361320980389913m\n",
            "Epoch:47, Acc:94.96, Precision: 83.43, Recall: 86.55, F1: 84.96 on Valid_set, Spend:0.059 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:48 completed, Total training's Loss: 2833684.3173828125, Spend: 1.36334308385849m\n",
            "Epoch:48, Acc:94.71, Precision: 82.08, Recall: 86.67, F1: 84.31 on Valid_set, Spend:0.055 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "Epoch:49 completed, Total training's Loss: 2835664.1127929688, Spend: 1.3544493595759073m\n",
            "Epoch:49, Acc:94.75, Precision: 82.33, Recall: 86.58, F1: 84.40 on Valid_set, Spend:0.059 minutes for evaluation\n",
            "--------------------------------------------------------------\n",
            "Epoch:49, Acc:96.35, Precision: 88.42, Recall: 90.60, F1: 89.49 on Test_set, Spend:0.063 minutes for evaluation\n",
            "--------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9635122941574554, 0.8949458978222161)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTqpiNtoBcKu"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}